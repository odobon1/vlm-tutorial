{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e64ea0",
   "metadata": {},
   "source": [
    "# From Pixels to Prompts: <br><small>A Crash Course on Zero-Shot Classification Using Vision-Language Models</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138a5d1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d25fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import open_clip\n",
    "from typing import List, Optional, Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from feat_eng import IMAGENET_CLASSES, IMAGENET_TEMPLATES\n",
    "from utils import DPATH_VALID, init_dataloader, batch_prec1, init_resnet50, init_vlm, print_eval_header\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9424b1",
   "metadata": {},
   "source": [
    "### Evaluation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "N_WORKERS  = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_inference(model:          nn.Module, \n",
    "                  dataloader:     DataLoader, \n",
    "                  device:         torch.device, \n",
    "                  model_name:     str, \n",
    "                  class_embs_txt: Optional[torch.Tensor] = None) -> None:\n",
    "    \n",
    "    # (1) Initialize some parameters for performance bookkeeping\n",
    "    n_samps   = 0\n",
    "    prec1_sum = 0.0\n",
    "    # (2) Iterate over validation data, loading up a batch of B images and corresponding targets at a time\n",
    "    for imgs_b, targs_enc_b in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "        \"\"\"\n",
    "        DELETE\n",
    "        imgs_b -------- Tensor(B, 3, 224, 224)\n",
    "        targs_enc_b --- Tensor(B)\n",
    "        \"\"\"\n",
    "        # (3) Number of samples in the current batch\n",
    "        B           = targs_enc_b.size(0)\n",
    "        # (4) Send batched images and targets tensors to GPU\n",
    "        imgs_b      = imgs_b.to(device, non_blocking=True)\n",
    "        targs_enc_b = targs_enc_b.to(device, non_blocking=True)\n",
    "\n",
    "        # (5) Perform batch inference with ResNet-50 or VLM. This will be explained next.\n",
    "        if model_name == \"ResNet-50\":\n",
    "            logits = batch_inference_res(model, imgs_b)\n",
    "        else:\n",
    "            logits = batch_inference_vlm(model, imgs_b, class_embs_txt)\n",
    "\n",
    "        # (x) Compute Top-1 Precision (Prec@1) for batch, update Prec@1 sum and sample count\n",
    "        prec1 = batch_prec1(logits, targs_enc_b)\n",
    "        prec1_sum += prec1.item() * B\n",
    "        n_samps += B\n",
    "\n",
    "    print(\n",
    "        f\"\",\n",
    "        f\"Prec@1: {prec1_sum / n_samps:.1%}\",\n",
    "        f\"\",\n",
    "        sep=\"\\n\"\n",
    "    )\n",
    "\n",
    "def batch_inference_res(model:  nn.Module, \n",
    "                        imgs_b: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    logits = model(imgs_b)\n",
    "\n",
    "    return logits\n",
    "\n",
    "def batch_inference_vlm(model:          nn.Module, \n",
    "                        imgs_b:         torch.Tensor, \n",
    "                        class_embs_txt: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    embs_img_b = model.encode_image(imgs_b)  # run batch of images through image encoder to produce embeddings, a D-dimensional embedding produced for each image\n",
    "    embs_img_b = F.normalize(embs_img_b, dim=1)  # embeddings are normalized to unit length\n",
    "    logits     = embs_img_b @ class_embs_txt\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db834c",
   "metadata": {},
   "source": [
    "### ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, img_pp     = init_resnet50(device)\n",
    "dataloader_resnet = init_dataloader(DPATH_VALID, img_pp, BATCH_SIZE, N_WORKERS)\n",
    "\n",
    "model_name = \"ResNet-50\"\n",
    "print_eval_header(model_name)\n",
    "run_inference(model, dataloader_resnet, device, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa03b7d",
   "metadata": {},
   "source": [
    "### Class Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_class_protos_text(\n",
    "    model:         nn.Module, \n",
    "    tokenizer:     Callable, \n",
    "    txts_class:    List[str], \n",
    "    txt_templates: List[str], \n",
    "    device:        torch.device) -> torch.Tensor:\n",
    "\n",
    "    protos = []\n",
    "\n",
    "    for txt in tqdm(txts_class, desc=\"Building class prototypes\"):\n",
    "\n",
    "        txts      = [temp.format(txt) for temp in txt_templates]\n",
    "        toks_txts = tokenizer(txts).to(device)\n",
    "\n",
    "        embs_txts = model.encode_text(toks_txts)\n",
    "        embs_txts = embs_txts / embs_txts.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        proto = embs_txts.mean(dim=0)\n",
    "        proto = proto / proto.norm()\n",
    "\n",
    "        protos.append(proto)\n",
    "\n",
    "    protos = torch.stack(protos, dim=1)\n",
    "    \n",
    "    return protos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f42397",
   "metadata": {},
   "source": [
    "### List Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e668b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e959084",
   "metadata": {},
   "source": [
    "### VLM Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90885b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLM_CONFIGS = [\n",
    "    (\"RN50\",                \"openai\", True,  \"CLIP ResNet-50 (224px)\"),\n",
    "    (\"ViT-B-32\",            \"openai\", True,  \"CLIP ViT-B/32 (224px)\"),\n",
    "    (\"ViT-B-16\",            \"openai\", True,  \"CLIP ViT-B/16 (224px)\"),\n",
    "    (\"ViT-L-14\",            \"openai\", True,  \"CLIP ViT-L/14 (224px)\"),\n",
    "    (\"ViT-L-14-336\",        \"openai\", True,  \"CLIP ViT-L/14 (336px)\"),\n",
    "    (\"ViT-B-16-SigLIP\",     \"webli\",  False, \"SigLIP ViT-B/16 (224px)\"),\n",
    "    (\"ViT-B-16-SigLIP-256\", \"webli\",  False, \"SigLIP ViT-B/16 (256px)\"),\n",
    "    (\"ViT-L-16-SigLIP-256\", \"webli\",  False, \"SigLIP ViT-L/16 (256px)\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304d59b",
   "metadata": {},
   "source": [
    "### Template Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENS_RAW      = [\"{}\"]\n",
    "ENS_STANDARD = [\"a photo of a {}.\"]\n",
    "ENS_CLIP80   = IMAGENET_TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c069e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ens_across_models(templates: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Benchmarks ensemble of templates across models specified in VLM_CONFIGS.\n",
    "    Templates + class labels + text encoder are used to create class prototypes.\n",
    "    Class prototypes are then used to performed zero-shot image classification.\n",
    "    \"\"\"\n",
    "\n",
    "    for model_id, pretrained, quick_gelu, model_name in VLM_CONFIGS:\n",
    "        print_eval_header(model_name)\n",
    "\n",
    "        model, img_pp, tokenizer = init_vlm(model_id, pretrained, quick_gelu, device)\n",
    "        \n",
    "        dataloader     = init_dataloader(DPATH_VALID, img_pp, BATCH_SIZE, N_WORKERS)\n",
    "        class_embs_txt = build_class_protos_text(model, tokenizer, IMAGENET_CLASSES, templates, device).to(device)\n",
    "\n",
    "        run_inference(model, dataloader, device, model_name, class_embs_txt=class_embs_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bff3c2",
   "metadata": {},
   "source": [
    "### Raw Label Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c73895",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_across_models(ENS_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc47f927",
   "metadata": {},
   "source": [
    "### Standard Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_across_models(ENS_STANDARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111e024",
   "metadata": {},
   "source": [
    "### CLIP 80 Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_across_models(ENS_CLIP80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
