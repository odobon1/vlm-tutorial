{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e64ea0",
   "metadata": {},
   "source": [
    "# From Pixels to Prompts: <br><small>A Crash Course on Zero-Shot Classification Using Vision-Language Models</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ab874",
   "metadata": {},
   "source": [
    "## Evaluation Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138a5d1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d25fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import open_clip\n",
    "from typing import List, Optional, Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from feat_eng import LABELS, TEMPLATES\n",
    "from utils import (\n",
    "    DPATH_VALID, \n",
    "    init_dataloader, \n",
    "    batch_prec1, \n",
    "    init_resnet50, \n",
    "    init_vlm, \n",
    "    print_eval_header,\n",
    ")\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9424b1",
   "metadata": {},
   "source": [
    "### Hardware Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "N_WORKERS  = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad0493",
   "metadata": {},
   "source": [
    "### Batched Inference: ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference_res(\n",
    "    model:  nn.Module, \n",
    "    imgs_b: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Run inference on mini-batch of images with ResNet-50 to produce logits.\n",
    "\n",
    "    Args:\n",
    "        model ---- ResNet-50\n",
    "        imgs_b --- Mini-batch of images, tensor of shape (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        Logits tensor of shape (B, L)\n",
    "    \"\"\"\n",
    "\n",
    "    logits = model(imgs_b)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d41395",
   "metadata": {},
   "source": [
    "### Batched Inference: VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference_vlm(\n",
    "    model:      nn.Module, \n",
    "    imgs_b:     torch.Tensor, \n",
    "    protos_txt: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Run batch inference on mini-batch of images using VLM image encoder + class prototype text embeddings to produce logits.\n",
    "\n",
    "    Args:\n",
    "        model -------- VLM\n",
    "        imgs_b ------- Mini-batch of images, tensor of shape (B, C, H, W)\n",
    "        protos_txt --- Class prototype text embeddings used by VLM to perform image classification, tensor of shape (L, D)\n",
    "\n",
    "    Returns:\n",
    "        Logits tensor of shape (B, L)\n",
    "    \"\"\"\n",
    "    # (x) ... tensor of shape (B, D)\n",
    "    embs_img_b = model.encode_image(imgs_b)  # run batch of images through image encoder to produce embeddings, a D-dimensional embedding produced for each image\n",
    "    embs_img_b = F.normalize(embs_img_b, dim=1)  # embeddings are normalized to unit length\n",
    "    # (x) embeddings are D-dimensional vectors on the unit hypersphere. Embeddings that point in a more similar direction will have higher cosine similarity.\n",
    "    logits     = embs_img_b @ protos_txt.T\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe5ee6",
   "metadata": {},
   "source": [
    "### Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_inference(\n",
    "    model:      nn.Module, \n",
    "    dataloader: DataLoader, \n",
    "    model_name: str, \n",
    "    protos_txt: Optional[torch.Tensor] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run evaluation on ImageNet1k validation set and report Top-1 Precision (Prec@1).\n",
    "    \n",
    "    Args:\n",
    "        model -------- ResNet-50 or VLM\n",
    "        dataloader --- Iterable yielding mini-batches of images and corresponding target encodings\n",
    "        model_name --- Model display name, used here to choose ResNet-50 or VLM batched inference function\n",
    "        protos_txt --- Class prototype text embeddings used by VLM to perform image classification, tensor of shape (L, D)\n",
    "    \"\"\"\n",
    "    # (x) Initialize some parameters for performance bookkeeping\n",
    "    n_samps   = 0\n",
    "    prec1_sum = 0.0\n",
    "    # (x) Iterate over validation data, loading up a batch of B images and corresponding targets at a time; imgs_b = tensor of shape (B, C, H, W), targs_b = tensor of shape (B)\n",
    "    for imgs_b, targs_b in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "        # (x) Send batched image and target tensors to GPU\n",
    "        imgs_b  = imgs_b.to(device, non_blocking=True)\n",
    "        targs_b = targs_b.to(device, non_blocking=True)\n",
    "\n",
    "        # (x) Perform batch inference with ResNet-50 or VLM to produce logits tensor of shape (B, L). If `model_name` is not \"ResNet-50\", we assume the model is a VLM.\n",
    "        if model_name == \"ResNet-50\":\n",
    "            logits = batch_inference_res(model, imgs_b)\n",
    "        else:\n",
    "            logits = batch_inference_vlm(model, imgs_b, protos_txt)\n",
    "\n",
    "        # (x) Compute Top-1 Precision (Prec@1) for batch, update per-sample Prec@1 sum and sample count\n",
    "        prec1 = batch_prec1(logits, targs_b)\n",
    "        # (x) Number of samples in the current batch\n",
    "        B     = targs_b.size(0)\n",
    "        prec1_sum += prec1.item() * B\n",
    "        n_samps += B\n",
    "\n",
    "    print(\n",
    "        f\"\",\n",
    "        f\"Prec@1: {prec1_sum / n_samps:.1%}\",\n",
    "        f\"\",\n",
    "        sep=\"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db834c",
   "metadata": {},
   "source": [
    "## ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ResNet-50\"\n",
    "print_eval_header(model_name)\n",
    "\n",
    "model, img_pp = init_resnet50(device)\n",
    "dataloader    = init_dataloader(DPATH_VALID, img_pp, BATCH_SIZE, N_WORKERS)\n",
    "\n",
    "run_inference(model, dataloader, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d335c19",
   "metadata": {},
   "source": [
    "## VLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789f8c4",
   "metadata": {},
   "source": [
    "### Zero-Shot Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46940f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_txts(\n",
    "    model:     nn.Module,\n",
    "    tokenizer: Callable,\n",
    "    txts:      List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encode text prompts.\n",
    "    P text prompts are encoded into P text embeddings and normalized to unit length.\n",
    "\n",
    "    Args:\n",
    "        model ------- VLM\n",
    "        tokenizer --- Tokenizer corresponding to VLM\n",
    "        txts -------- Text prompts\n",
    "\n",
    "    Returns:\n",
    "        Text embeddings tensor of shape (P, D)\n",
    "    \"\"\"\n",
    "    # (x) token embedding tensor of shape (L, T)\n",
    "    toks_txt = tokenizer(txts).to(device)\n",
    "\n",
    "    # (x) tensor of shape (L, D)\n",
    "    embs_txt = model.encode_text(toks_txt)\n",
    "    # (x) normalize text embeddings to unit length\n",
    "    embs_txt = F.normalize(embs_txt, dim=1)\n",
    "\n",
    "    return embs_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bd7ea",
   "metadata": {},
   "source": [
    "### List Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31213369",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088cde7",
   "metadata": {},
   "source": [
    "### Flagship CLIP Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea932b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id   = \"ViT-L-14-336\"\n",
    "pretrained = \"openai\"\n",
    "quick_gelu = True\n",
    "model_name = \"CLIP ViT-L/14 (336px)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8b722",
   "metadata": {},
   "source": [
    "### Zero-Shot Classification: Raw Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_eval_header(model_name)\n",
    "\n",
    "model, img_pp, tokenizer = init_vlm(model_id, pretrained, quick_gelu, device)\n",
    "\n",
    "dataloader = init_dataloader(DPATH_VALID, img_pp, BATCH_SIZE, N_WORKERS)\n",
    "protos_txt = encode_txts(model, tokenizer, LABELS).to(device)  # tensor of shape (L, D)\n",
    "\n",
    "run_inference(model, dataloader, model_name, protos_txt=protos_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e17251e",
   "metadata": {},
   "source": [
    "### Zero-Shot Classification: Standard Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acefe275",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_eval_header(model_name)\n",
    "\n",
    "model, img_pp, tokenizer = init_vlm(model_id, pretrained, quick_gelu, device)\n",
    "\n",
    "dataloader = init_dataloader(DPATH_VALID, img_pp, BATCH_SIZE, N_WORKERS)\n",
    "prompts    = [\"a photo of a {}.\".format(label) for label in LABELS]\n",
    "protos_txt = encode_txts(model, tokenizer, prompts).to(device)\n",
    "\n",
    "run_inference(model, dataloader, model_name, protos_txt=protos_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa03b7d",
   "metadata": {},
   "source": [
    "### Prompt Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED BETTER FUNCTION NAME FOR THIS\n",
    "@torch.no_grad()\n",
    "def build_ensemble_prototypes(\n",
    "    model:     nn.Module,\n",
    "    tokenizer: Callable,\n",
    "    labels:    List[str],\n",
    "    temps:     List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build per-class prototype embeddings in the text space.\n",
    "    A list of templates is applied to each class label. Resulting prompts are encoded with the VLM text encoder and normalized\n",
    "    to unit length to produce per-template text embeddings. These embeddings are then averaged and normalized to unit length\n",
    "    to produce class prototype. This process is performed for each class.\n",
    "\n",
    "    Args:\n",
    "        model ------- VLM\n",
    "        tokenizer --- Tokenizer corresponding to VLM\n",
    "        labels ------ Class labels\n",
    "        temps ------- Prompt templates with a \"{}\" placeholder for class labels\n",
    "\n",
    "    Returns:\n",
    "        Class prototypes tensor of shape (L, D)\n",
    "    \"\"\"\n",
    "    # (x) we initialize a list to cache prototype embeddings (\"cache\" correctly used here?)\n",
    "    protos = []\n",
    "\n",
    "    # (x) we iterate over class labels\n",
    "    for txt in tqdm(labels, desc=\"Building prototypes\"):\n",
    "\n",
    "        txts     = [temp.format(txt) for temp in temps]\n",
    "        embs_txt = encode_txts(model, tokenizer, txts)  # tensor of shape (P, D)\n",
    "\n",
    "        # (x) take the average of the embeddings to produce a prototype embedding of dimension (D)\n",
    "        proto = embs_txt.mean(dim=0)  # tensor of shape (D)\n",
    "        # (x) normalize prototype embedding to unit length\n",
    "        proto = F.normalize(proto, dim=0)\n",
    "\n",
    "        protos.append(proto)\n",
    "\n",
    "    # (x) we convert the list of 1D tensors into a 2D tensor of shape (L, D)\n",
    "    protos = torch.stack(protos)\n",
    "\n",
    "    return protos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e959084",
   "metadata": {},
   "source": [
    "### VLM Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90885b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLM_CONFIGS = [\n",
    "    (\"RN50\",                \"openai\", True,  \"CLIP ResNet-50 (224px)\"),\n",
    "    (\"ViT-B-32\",            \"openai\", True,  \"CLIP ViT-B/32 (224px)\"),\n",
    "    (\"ViT-B-16\",            \"openai\", True,  \"CLIP ViT-B/16 (224px)\"),\n",
    "    (\"ViT-L-14\",            \"openai\", True,  \"CLIP ViT-L/14 (224px)\"),\n",
    "    (\"ViT-L-14-336\",        \"openai\", True,  \"CLIP ViT-L/14 (336px)\"),\n",
    "    (\"ViT-B-16-SigLIP\",     \"webli\",  False, \"SigLIP ViT-B/16 (224px)\"),\n",
    "    (\"ViT-B-16-SigLIP-256\", \"webli\",  False, \"SigLIP ViT-B/16 (256px)\"),\n",
    "    (\"ViT-L-16-SigLIP-256\", \"webli\",  False, \"SigLIP ViT-L/16 (256px)\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111e024",
   "metadata": {},
   "source": [
    "### Zero-Shot Classification: OpenAI ImageNet1k Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de780695",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id, pretrained, quick_gelu, model_name in VLM_CONFIGS:\n",
    "    # (brackets, or maybe up top) this code should look familiar. It is the code we used in the first two VLM evaluations, except that template ensembling is used to construct the class prototype text embeddings.\n",
    "    print_eval_header(model_name)\n",
    "    \n",
    "    model, img_pp, tokenizer = init_vlm(model_id, pretrained, quick_gelu, device)\n",
    "    \n",
    "    dataloader = init_dataloader(DPATH_VALID, img_pp, BATCH_SIZE, N_WORKERS)\n",
    "    protos_txt = build_ensemble_prototypes(model, tokenizer, LABELS, TEMPLATES).to(device)  # tensor of shape (L, D)\n",
    "\n",
    "    run_inference(model, dataloader, model_name, protos_txt=protos_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
